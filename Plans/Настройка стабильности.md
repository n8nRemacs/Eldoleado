# Настройка отказоустойчивости MCP серверов

## Проблема

Текущая архитектура имеет single point of failure:
- Если Nginx падает — все сервисы недоступны
- Если MCP сервер падает — канал недоступен
- Нет автоматического переключения на резерв

## Целевая архитектура

```
                         ┌──────────────────┐
                         │   Virtual IP     │
                         │  45.144.177.100  │
                         │  (Floating IP)   │
                         └────────┬─────────┘
                                  │
              ┌───────────────────┴───────────────────┐
              ▼                                       ▼
    ┌──────────────────┐                   ┌──────────────────┐
    │   HAProxy #1     │◄────Keepalived───▶│   HAProxy #2     │
    │   MASTER         │     (heartbeat)   │   BACKUP         │
    │   Server A       │                   │   Server B       │
    └────────┬─────────┘                   └────────┬─────────┘
             │                                      │
             └──────────────────┬───────────────────┘
                                │
         ┌──────────────────────┼──────────────────────┐
         ▼                      ▼                      ▼
   ┌───────────┐          ┌───────────┐          ┌───────────┐
   │  MCP #1   │          │  MCP #2   │          │  MCP #3   │
   │ (primary) │          │ (backup)  │          │ (backup)  │
   └───────────┘          └───────────┘          └───────────┘
```

## Как это работает

### 1. Virtual IP (Floating IP)
- Один IP адрес, который "плавает" между серверами
- DNS указывает на этот IP: `mcp.eldoleado.ru → 45.144.177.100`
- Клиенты всегда обращаются к одному адресу

### 2. Keepalived
- Демон, работающий на обоих HAProxy серверах
- Посылает heartbeat между серверами каждые 1-2 секунды
- Если MASTER не отвечает 3 секунды → BACKUP забирает Virtual IP
- Переключение происходит за **1-3 секунды**

### 3. HAProxy
- Балансировщик нагрузки с health checks
- Проверяет `/health` endpoint каждые 3 секунды
- Если MCP сервер не отвечает → исключает из ротации
- Переключение на резервный MCP за **3-10 секунд**

## Сравнение вариантов

| Параметр | Nginx | HAProxy + Keepalived |
|----------|-------|----------------------|
| Single point of failure | Да | Нет |
| Время переключения | 10-30 сек | 1-3 сек |
| Health checks | Базовые | Продвинутые |
| Сложность настройки | Простая | Средняя |
| Мониторинг | Базовый | Встроенная статистика |
| SSL termination | Да | Да |
| WebSocket support | Да | Да |

## Требования к инфраструктуре

### Минимум (для production)
- **2 сервера** для HAProxy + Keepalived
- **2 сервера** для MCP (или контейнеры на тех же серверах)
- **1 Floating IP** от хостинга

### Рекомендуемая конфигурация
```
Server A (HAProxy MASTER + MCP Primary):
  - HAProxy: порт 80, 443
  - MCP-telegram: порт 8761
  - MCP-whatsapp: порт 8762
  - ...

Server B (HAProxy BACKUP + MCP Backup):
  - HAProxy: порт 80, 443 (standby)
  - MCP-telegram: порт 8761 (backup)
  - MCP-whatsapp: порт 8762 (backup)
  - ...
```

## План внедрения

### Этап 1: Подготовка (1 час)
- [ ] Заказать второй сервер или VPS
- [ ] Заказать Floating IP у хостинга
- [ ] Настроить SSH доступ между серверами

### Этап 2: Установка HAProxy (30 мин на сервер)
```bash
# На обоих серверах
apt update && apt install -y haproxy

# Проверка
haproxy -v
```

### Этап 3: Настройка HAProxy (30 мин)

Конфиг `/etc/haproxy/haproxy.cfg`:

```haproxy
global
    log /dev/log local0
    maxconn 4096
    user haproxy
    group haproxy
    daemon

    # SSL settings
    ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256
    ssl-default-bind-options no-sslv3 no-tlsv10 no-tlsv11

defaults
    log     global
    mode    http
    option  httplog
    option  dontlognull
    option  http-server-close
    option  forwardfor
    timeout connect 5s
    timeout client  30s
    timeout server  30s

    # Retry на другой сервер при ошибке
    retries 3
    option redispatch

# Статистика HAProxy (для мониторинга)
listen stats
    bind *:8404
    stats enable
    stats uri /stats
    stats refresh 10s
    stats auth admin:CHANGE_THIS_PASSWORD

# Frontend - принимает все запросы
frontend http_front
    bind *:80
    bind *:443 ssl crt /etc/haproxy/certs/eldoleado.pem

    # Редирект HTTP → HTTPS
    http-request redirect scheme https unless { ssl_fc }

    # Маршрутизация по path
    acl is_telegram path_beg /webhook/telegram /api/telegram
    acl is_whatsapp path_beg /webhook/whatsapp /api/whatsapp
    acl is_vk path_beg /webhook/vk /api/vk
    acl is_instagram path_beg /webhook/instagram /api/instagram
    acl is_avito path_beg /webhook/avito /api/avito
    acl is_max path_beg /webhook/max /api/max

    use_backend telegram_backend if is_telegram
    use_backend whatsapp_backend if is_whatsapp
    use_backend vk_backend if is_vk
    use_backend instagram_backend if is_instagram
    use_backend avito_backend if is_avito
    use_backend max_backend if is_max

    default_backend default_backend

# Backend для каждого канала
backend telegram_backend
    balance roundrobin
    option httpchk GET /health
    http-check expect status 200

    # Primary сервер
    server mcp-telegram-1 10.0.0.1:8761 check inter 3s fall 2 rise 2
    # Backup сервер (используется только если primary недоступен)
    server mcp-telegram-2 10.0.0.2:8761 check inter 3s fall 2 rise 2 backup

backend whatsapp_backend
    balance roundrobin
    option httpchk GET /health
    http-check expect status 200

    server mcp-whatsapp-1 10.0.0.1:8762 check inter 3s fall 2 rise 2
    server mcp-whatsapp-2 10.0.0.2:8762 check inter 3s fall 2 rise 2 backup

backend vk_backend
    balance roundrobin
    option httpchk GET /health
    http-check expect status 200

    server mcp-vk-1 10.0.0.1:8767 check inter 3s fall 2 rise 2
    server mcp-vk-2 10.0.0.2:8767 check inter 3s fall 2 rise 2 backup

backend instagram_backend
    balance roundrobin
    option httpchk GET /health
    http-check expect status 200

    server mcp-instagram-1 10.0.0.1:8766 check inter 3s fall 2 rise 2
    server mcp-instagram-2 10.0.0.2:8766 check inter 3s fall 2 rise 2 backup

backend avito_backend
    balance roundrobin
    option httpchk GET /health
    http-check expect status 200

    server mcp-avito-1 10.0.0.1:8765 check inter 3s fall 2 rise 2
    server mcp-avito-2 10.0.0.2:8765 check inter 3s fall 2 rise 2 backup

backend max_backend
    balance roundrobin
    option httpchk GET /health
    http-check expect status 200

    server mcp-max-1 10.0.0.1:8763 check inter 3s fall 2 rise 2
    server mcp-max-2 10.0.0.2:8763 check inter 3s fall 2 rise 2 backup

backend default_backend
    server default 127.0.0.1:8080
```

### Этап 4: Установка Keepalived (20 мин)
```bash
# На обоих серверах
apt install -y keepalived
```

### Этап 5: Настройка Keepalived

**На MASTER сервере** `/etc/keepalived/keepalived.conf`:
```
vrrp_script check_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
    weight 2
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0          # Сетевой интерфейс
    virtual_router_id 51
    priority 101            # Выше = предпочтительнее
    advert_int 1            # Heartbeat каждую секунду

    authentication {
        auth_type PASS
        auth_pass CHANGE_THIS_SECRET
    }

    virtual_ipaddress {
        45.144.177.100/24   # Floating IP
    }

    track_script {
        check_haproxy
    }
}
```

**На BACKUP сервере** `/etc/keepalived/keepalived.conf`:
```
vrrp_script check_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
    weight 2
}

vrrp_instance VI_1 {
    state BACKUP
    interface eth0
    virtual_router_id 51
    priority 100            # Ниже чем у MASTER
    advert_int 1

    authentication {
        auth_type PASS
        auth_pass CHANGE_THIS_SECRET
    }

    virtual_ipaddress {
        45.144.177.100/24
    }

    track_script {
        check_haproxy
    }
}
```

### Этап 6: Запуск и проверка (15 мин)

```bash
# На обоих серверах
systemctl enable haproxy keepalived
systemctl start haproxy keepalived

# Проверка
systemctl status haproxy
systemctl status keepalived

# Проверка Virtual IP (должен быть на MASTER)
ip addr show eth0 | grep 45.144.177.100

# Тест failover - остановить HAProxy на MASTER
systemctl stop haproxy

# Virtual IP должен перейти на BACKUP за 1-3 секунды
# На BACKUP:
ip addr show eth0 | grep 45.144.177.100
```

### Этап 7: Настройка DNS (5 мин)
```
mcp.eldoleado.ru    → 45.144.177.100 (Floating IP)
```

## Мониторинг

### HAProxy Statistics
Доступно по адресу: `http://FLOATING_IP:8404/stats`

Показывает:
- Статус каждого backend сервера (UP/DOWN)
- Количество активных соединений
- Время ответа
- Количество ошибок

### Алерты (опционально)

Добавить в `/etc/keepalived/keepalived.conf`:
```
vrrp_instance VI_1 {
    ...
    notify_master "/usr/local/bin/notify.sh MASTER"
    notify_backup "/usr/local/bin/notify.sh BACKUP"
    notify_fault "/usr/local/bin/notify.sh FAULT"
}
```

Скрипт `/usr/local/bin/notify.sh`:
```bash
#!/bin/bash
STATE=$1
HOSTNAME=$(hostname)

# Отправка в Telegram
curl -s -X POST "https://api.telegram.org/bot$BOT_TOKEN/sendMessage" \
    -d "chat_id=$ADMIN_CHAT_ID" \
    -d "text=HAProxy $HOSTNAME: $STATE"
```

## Синхронизация состояния между серверами

### Проблема
При failover новый MCP сервер должен знать о зарегистрированных аккаунтах.

### Решение
Уже реализовано! Аккаунты хранятся в:
1. **Redis** (primary) — общий для всех инстансов
2. **PostgreSQL** (backup) — общий для всех

При старте MCP сервер загружает аккаунты из Redis:
```python
async def lifespan(app: FastAPI):
    await init_storage()
    accounts = await load_accounts(CHANNEL_NAME)  # Из Redis
    for acc in accounts:
        # Восстановить клиенты
```

### Требования
- Все MCP серверы должны использовать один Redis
- Redis URL: `redis://185.221.214.83:6379`
- Redis тоже нужно реплицировать (Redis Sentinel или Redis Cluster)

## Redis High Availability (опционально)

Для полной отказоустойчивости Redis:

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   Redis     │     │   Redis     │     │   Redis     │
│   Master    │────▶│   Slave 1   │────▶│   Slave 2   │
└──────┬──────┘     └─────────────┘     └─────────────┘
       │
       ▼
┌─────────────────────────────────────────────────────┐
│              Redis Sentinel (3 instances)           │
│  Мониторит Master, при падении промоутит Slave      │
└─────────────────────────────────────────────────────┘
```

## Чек-лист перед production

- [ ] Два сервера с HAProxy + Keepalived
- [ ] Floating IP настроен
- [ ] SSL сертификаты скопированы на оба сервера
- [ ] MCP серверы запущены на обоих серверах
- [ ] Health checks работают
- [ ] Тест failover пройден
- [ ] Мониторинг настроен
- [ ] Алерты настроены
- [ ] DNS обновлён на Floating IP
- [ ] Redis доступен с обоих серверов

## Время восстановления

| Сценарий | Время |
|----------|-------|
| MCP сервер упал | 3-10 сек |
| HAProxy упал | 1-3 сек |
| Весь сервер упал | 1-3 сек |
| Сеть между серверами упала | 1-3 сек (split-brain protection) |

## Стоимость

| Компонент | Примерная стоимость |
|-----------|---------------------|
| Второй VPS (2 CPU, 4GB RAM) | ~1500 руб/мес |
| Floating IP | ~200-500 руб/мес |
| Redis Sentinel (если нужен) | Включён в VPS |

**Итого:** ~2000 руб/мес за полную отказоустойчивость
